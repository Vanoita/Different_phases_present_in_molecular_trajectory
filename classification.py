# -*- coding: utf-8 -*-
"""midsem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ruBldlqPlVyBEOw3NSpf071lvaM1GrwW

# Introduction

Given data contains molecular trajectories of a molecule. A molecular dynamics trajectory for a set of N atoms is essentially
an ordered set of 3N-dimensional vectors. We perform analytical steps to analyze the number of phases present in the trajectory. We use dimensionality reduction to reduce the dependent parameters from 3D to 2D. Then we use unsupervised clustering methods to classify the datset into np.of phases.

# Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans
import matplotlib as mp

"""# Importing Data

Mounting google drive
"""

from google.colab import drive
drive._mount('/content/drive')

file_name = open('/content/drive/MyDrive/Polymer.txt','r')
seq = file_name.readlines()

"""Cleaning the data and acquiring important data"""

c=0
for i in range(0,12060):
  for j in range(0,9):
    del seq[c]
  c=c+100;

id=[]
x=[]
y=[]
z=[]

for line in seq:
        Type = line.split(" ")
        id.append(int(Type[0]))
        x.append(float(Type[3]))
        y.append(float(Type[4]))
        z.append(float(Type[5]))

"""Converting arrays into dataframs"""

df=pd.DataFrame({'id':id,'x':x,'y':y,'z':z})

df.head()

"""# Scaling the data"""

df = StandardScaler().fit_transform(df)
df = pd.DataFrame(data = df
             , columns = ['id', 'x','y','z'])

df.head()

"""# Principal component analysis"""

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(df)
principaldf = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2'])

principaldf.head()

fig = plt.figure(figsize = (10,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)
plt.scatter(principaldf['principal component 1'], principaldf['principal component 2'])
ax.grid()

"""# Clustering - KMeans

Finding no.of clusters using Elbow method
"""

from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans
model = KElbowVisualizer(KMeans(), k=10)
model.fit(principaldf)
model.show()

"""**Model implementation**

From the above analysis we found out that the optimal number of clusters is 5.
"""

kmeans = KMeans(5)
kmeans.fit(principaldf)

principaldf['kmeans']=kmeans.labels_
principaldf.head()

"""Visualising model"""

min_val, max_val = 0.34,1.0
n = 10
orig_cmap = plt.cm.Blues
colors = orig_cmap(np.linspace(min_val, max_val, n))
cmap = mp.colors.LinearSegmentedColormap.from_list("mycmap", colors)

plt.scatter(principaldf['principal component 1'],principaldf['principal component 2'],c=principaldf['kmeans'],cmap=cmap)
plt.show()

"""# Conclusions

We see that Kmeans algorithm has calculated the no.of efficient clusters as 5.

# References

https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60

https://pubs.acs.org/doi/pdf/10.1021/acs.chemrev.0c01195

https://www.frontiersin.org/articles/10.3389/fmolb.2019.00046/full

https://realpython.com/k-means-clustering-python/
"""
